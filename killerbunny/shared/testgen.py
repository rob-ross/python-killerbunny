#  File: testgen.py
#  Copyright (c) 2025 Robert L. Ross
#  All rights reserved.
#  Open-source license to come.
#  Created by: Robert L. Ross
#
#

# testgen.py

"""Functions for creating test data files for the Lexer, Parser, and Evaluator unit tests"""
import json
from pathlib import Path
from typing import cast

from killerbunny.evaluating.evaluator import JPathEvaluator
from killerbunny.evaluating.runtime_result import RuntimeResult
from killerbunny.evaluating.value_nodes import VNodeList
from killerbunny.lexing.lexer import JPathLexer
from killerbunny.parsing.node_type import ASTNode
from killerbunny.parsing.parse_result import ParseResult
from killerbunny.parsing.parser import JPathParser
from killerbunny.shared.constants import UTF8, ONE_MEBIBYTE, JPATH_DATA_SEPARATOR, ROOT_JSON_VALUE_KEY
from killerbunny.shared.context import Context
from killerbunny.shared.json_type_defs import JSON_ValueType

"""
Lexer test file generation: (remember, we're just testing the lexer. Paths don't have to resolve to an actual json value)

1. Input the Path of  a json value in a text file (.json file).
2. Generate a VNode for every member value node in the json value (i.e, for the Root node)
3. Run the Lexer for every VNode Normalized Path. The path string followed by the separator then the Lexer's token
 output list is an entry in the test file.
 
 4. path enhancements. For each subpath,
    A. use membername shorthand (iff possible) to substitute for bracketed name selector. (Use MemberNameShorthand regex
    to see if the name can exist as a mnsh. If so, add that alternate path
    B. For each subpath, add variation using a wildcard selector. both as a member name shorthand and a bracketed wildcard
    selector.
    C. For lists, after visiting each element in the list to generate the subpaths for each, randomly generate a slice selector
    based on the size of the array and generate a subpath for the slice selector
    D. For each new psth segment added, also add a Descendant segment as well.
"""

_MODULE_DIR = Path(__file__).parent

def load_obj_from_json_file(input_file: Path) -> JSON_ValueType:
    """Return the  json object from the json file in the argument.
    Intended for a json file with a single object for testing and debugging. """
    with open(input_file, "rb", buffering=ONE_MEBIBYTE) as in_file:
        json_str = in_file.read()
        return cast(JSON_ValueType, json.loads(json_str))
    
####################################################################
# LEXER TEST GENERATION
####################################################################

def tokenize_jpath_str(file_name:str, jpath_query_str: str) -> str:
    """ Run the `jpath_query_str` through the lexer and return a string representation of the generated Token list. """
    lexer = JPathLexer(file_name, jpath_query_str)
    tokens, error = lexer.tokenize()
    result_str: str = ""
    if error:
        result_str = error.as_test_string()
    else:
        if tokens:
            result_str = ', '.join( t.__testrepr__() for t in tokens)
    return result_str
    

# Given an input file path, return a list of tokens generated by the lexer for each line in the input file.
# Each line of the input file is a json path query string. This method will pass each string to the lexer and
# receive from it a list of scanned tokens corresponding to the input string. This method returns a list of two-item
# tuples. The first tuple item is the original json path query string. The second tuple item is a string representation
# of the list of generated tokens. The string representation of each token is generated by calling __testrepr__() on
# each token. Comments in the input file are ignored. If a line starts with #, it is ignored. Note that # is a valid
# character that may be part of a member name or seach string, so inline comments are not supported.
def generate_lexer_path_values(input_path: Path) -> list[ tuple[ str, str]  ]:
    """For each line in the input file of json path query strings,  return a list of tokens generated by the lexer.
    
    Each line of the input file is a json path query string. This method will pass each string to the lexer and
    receive from it a list of scanned tokens corresponding to the input string, or an error message if the lexer failed
    to scan the input string.
    This method returns a list of two-item tuples. The first tuple item is the original json path query string.
    The second tuple item is a string representation of the list of generated tokens, or a lexer error message.
    The string representation of each token is generated by calling __testrepr__() on each token.
    Lines that start with a # (line comment) are ignored, as well as blank lines.
    Note that # is a valid character that may be part of a member name or search string, so end-of-line comments
    are not supported.

    """
    result_list: list[ tuple[ str, str] ] = []
    file_name: str = input_path.name
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_str: str = tokenize_jpath_str(file_name, line_stripped)
            result_list.append( (line_stripped, result_str) )
    return result_list

FRAGILE_TEST_DIR = _MODULE_DIR / '../../../../tests/test_jpath_interpreter'
FRAGILE_TEST_DIR_PATH = Path(FRAGILE_TEST_DIR)

def process_lexer_paths(input_dir: Path, suffix: str, generate_test_files: bool = False, quiet:bool = False)-> None:
    """Given a directory path and a file suffix, run generate_lexer_path_values() for each file in the directory
    that ends with `suffix`. No directory recursion is done. If `generate_test_files` is True, will create a test file
    for each matching input file and save it in the tests.../.../lexing/jpath_token_files dir. Test files contain a
    json path query string followed by  JPATH_DATA_SEPARATOR and the string representation of the list of generated tokens.
    If `quiet` is True, omits writing most output to the consoled, except for warning or error messages.
    
    This is a utility function to help create test data for lexer unit testing. This should be used with caution to avoid
    accidentally ovewriting existing test files. However, the open() call for creating the test file uses the 'x' mode
    flag, so it will report an error if the file alredy exists.
    """
    if not quiet:
        print(f"*** Processing jpathl files for the Lexer")
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    output_dir = FRAGILE_TEST_DIR_PATH / "incubator/jpath" / "lexing" / "jpath_token_files"
    if generate_test_files:
        # ensure test directory exists
        if not quiet:
            print()
            print(f"Generating test files in {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
    generated_files: list[str] = []
    for file in input_dir.iterdir():
        if file.name.endswith(suffix):
            if not quiet:
                print()
                print(f"Processing '{file}'")
            file_results: list[ tuple[ str, str ]] = generate_lexer_path_values(file)
            if not file_results:
                print(f"Warning: file '{file}' produced no results. Is it empty?")
                continue
            
            if file_results and not quiet:
                print(f"{input_dir.name}/{file.name} results:")
                for jpath, tokens in file_results:
                    print(f"{jpath}{JPATH_DATA_SEPARATOR}{tokens}")
                print("-" * 40)
                
            if generate_test_files:
                outfile_path = output_dir / ( file.stem + ".jpath_tokens" )
                if not quiet: print(f"Generating test file '{outfile_path}'")
                generated_files.append( outfile_path.name )
                with open(outfile_path, "x", encoding=UTF8, buffering=ONE_MEBIBYTE) as outfile:
                    outfile.write(f"# file_name: {outfile_path.name}\n")
                    for jpath, tokens in file_results:
                        outfile.write(f"{jpath}{JPATH_DATA_SEPARATOR}{tokens}\n")
                
    if generated_files and not quiet:
        print()
        print(f"Generated a total of {len(generated_files)} lexer test files:")
        for line in sorted(generated_files):
            print(f"  {line}" )


####################################################################
# PARSER TEST GENERATION
####################################################################

def parse_jpath_str(file_name:str, jpath_query_str: str) -> str:
    """Lex and parse the json path query string and return a string representation of the generated AST from the parser."""
    lexer = JPathLexer(file_name, jpath_query_str)
    tokens, error = lexer.tokenize()
    if error:
        result_str = error.as_test_string()
        return result_str
    
    parser = JPathParser(tokens)
    result: ParseResult =  parser.parse()
    if result.error:
        result_str = result.error.as_test_string()
        return result_str
    
    ast: ASTNode| None  = result.node
    return str(ast)

def generate_parser_path_values(input_path: Path) -> list[ tuple[ str, str]  ]:
    result_list: list[ tuple[ str, str] ] = []
    file_name: str = input_path.name
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_str: str = parse_jpath_str(file_name, line_stripped)
            result_list.append( (line_stripped, result_str) )
    return result_list



def process_parser_paths(input_dir: Path,
                         suffix: str,
                         output_dir: Path | None = None,
                         generate_test_files: bool = False,
                         quiet:bool = False)-> None:
    
    
    #output_dir = FRAGILE_TEST_DIR_PATH / "incubator/jpath" / "lexing" / "jpath_token_files"
    if not quiet:
        print(f"*** Processing jpathl files for the Parser")
        
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    if generate_test_files and output_dir is None:
        raise ValueError(f"`generate_test_files` is  True yet no output directory was specified.")
    
    if generate_test_files and output_dir is not None:
        # ensure test directory exists
        output_dir.mkdir(parents=True, exist_ok=True)
        if not quiet:
            print()
            print(f"Generating test files in {output_dir}")
    
    generated_files: list[str] = []
    for file in input_dir.iterdir():
        if file.name.endswith(suffix):
            if not quiet:
                print()
                print(f"Processing '{file}'")
            file_results: list[ tuple[ str, str]  ] = generate_parser_path_values(file)
            if not file_results:
                print(f"Warning: file '{file}' produced no results. Is it empty?")
                continue
            
            if file_results and not quiet:
                print(f"{input_dir.name}/{file.name} results:")
                for jpath, ast in file_results:
                    print(f"{jpath}{JPATH_DATA_SEPARATOR}{ast}")
                print("-" * 40)
            
            if generate_test_files and output_dir:
                outfile_path = output_dir / ( file.stem + ".jpath_ast" )
                if not quiet: print(f"Generating test file '{outfile_path}'")
                generated_files.append( outfile_path.name )
                with open(outfile_path, "x", encoding=UTF8, buffering=ONE_MEBIBYTE) as outfile:
                    outfile.write(f"# file_name: {outfile_path.name}\n")
                    for jpath, ast in file_results:
                        outfile.write(f"{jpath}{JPATH_DATA_SEPARATOR}{ast}\n")
    
    if generated_files and not quiet:
        print()
        print(f"Generated a total of {len(generated_files)} parser test files:")
        for line in sorted(generated_files):
            print(f"  {line}" )


####################################################################
# EVALUATOR TEST GENERATION
####################################################################

def evaluate_jpath_str(file_name:str, jpath_query_str: str, json_value:JSON_ValueType) -> VNodeList:
    """Lex and parse the json path query string and evalutate it with the JSON value in the argument.
    Return a VNodeList"""
    lexer = JPathLexer(file_name, jpath_query_str)
    tokens, error = lexer.tokenize()
    if error:
        result_str = error.as_test_string()
        raise ValueError(result_str)
    
    parser = JPathParser(tokens)
    result: ParseResult =  parser.parse()
    if result.error:
        result_str = result.error.as_test_string()
        raise ValueError(result_str)
    
    ast_node = result.node
    if ast_node is None:
        raise ValueError(f"Parser returned an empty AST for query string: {jpath_query_str}")
    context = Context('<root>')
    context.set_symbol(ROOT_JSON_VALUE_KEY, json_value)
    evaluator = JPathEvaluator()
    rt_result:RuntimeResult = evaluator.visit(ast_node, context)
    if rt_result.error:
        raise ValueError(f"Evaluator returned Error: {rt_result.error} for query string: {jpath_query_str}")
    if rt_result.value is not None:
       return rt_result.value
    else:
        raise RuntimeError(f"Evaluator returned null value for query string: {jpath_query_str}")

def generate_evaluator_path_nodelist(input_path: Path, json_value: JSON_ValueType) -> list[ tuple[ str, VNodeList]  ]:
    result_list: list[ tuple[ str, VNodeList] ] = []
    file_name: str = input_path.name
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_nodelist: VNodeList = evaluate_jpath_str(file_name, line_stripped, json_value)
            result_list.append( (line_stripped, result_nodelist) )
    return result_list

##################################################################################################################


def t1()-> None:
    """TEMP CODE"""
    file1 = Path(FRAGILE_TEST_DIR) / "jpath/rfc9535_examples/table_02.jpathl"
    json_file = Path(FRAGILE_TEST_DIR) / "jpath/rfc9535_examples/figure.1.json"
    json_value: JSON_ValueType = load_obj_from_json_file(json_file)
    result = generate_evaluator_path_nodelist(file1,json_value)
    for jpath, nodelist in result:
        print(f"{jpath}")
        for node in nodelist:
            print(f"{node}")
        print()
        

def rename_file_suffix(input_dir: Path, from_suffix: str, to_suffix: str)-> None:
    """ TEMP CODE """
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    for file in input_dir.iterdir():
        if file.name.endswith(from_suffix):
            file.rename(file.with_suffix(to_suffix))

def create_lexer_test_files() -> None:
    """PRODUCTION CODE"""
    input_dir = Path(FRAGILE_TEST_DIR) / "jpath/rfc9535_examples/"
    process_lexer_paths(input_dir, "jpathl", generate_test_files=True)
    
def create_parser_test_files() -> None:
    """PRODUCTION CODE"""
    input_dir  = FRAGILE_TEST_DIR_PATH / "jpath/rfc9535_examples/"
    output_dir = FRAGILE_TEST_DIR_PATH / "jpath/parsing/jpath_parser_files"
    process_parser_paths(input_dir=input_dir, suffix="jpathl", output_dir=output_dir, generate_test_files=True)

def create_all_test_files() -> None:
    """PRODUCTION CODE"""
    create_lexer_test_files()
    create_parser_test_files()

def main() -> None:
    t1()
    #create_parser_test_files()
    
if __name__ == '__main__':
    main()
