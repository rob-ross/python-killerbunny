#  File: testgen.py
#  Copyright (c) 2025 Robert L. Ross
#  All rights reserved.
#  Open-source license to come.
#  Created by: Robert L. Ross
#
#


"""Functions for creating test data files for the Lexer, Parser, and Evaluator unit tests"""
import json
from dataclasses import dataclass, asdict, is_dataclass
from pathlib import Path
from typing import cast, Any

from killerbunny.evaluating.evaluator import JPathEvaluator
from killerbunny.evaluating.runtime_result import RuntimeResult
from killerbunny.evaluating.value_nodes import VNodeList
from killerbunny.lexing.lexer import JPathLexer
from killerbunny.lexing.tokens import Token
from killerbunny.parsing.node_type import ASTNode
from killerbunny.parsing.parse_result import ParseResult
from killerbunny.parsing.parser import JPathParser
from killerbunny.shared.constants import UTF8, ONE_MEBIBYTE, JPATH_DATA_SEPARATOR, ROOT_JSON_VALUE_KEY
from killerbunny.shared.context import Context
from killerbunny.shared.errors import Error
from killerbunny.shared.json_type_defs import JSON_ValueType

"""
Lexer test file generation: (remember, we're just testing the lexer. Paths don't have to resolve to an actual json value)

1. Input the Path of  a json value in a text file (.json file).
2. Generate a VNode for every member value node in the json value (i.e, for the Root node)
3. Run the Lexer for every VNode Normalized Path. The path string followed by the separator then the Lexer's token
 output list is an entry in the test file.
 
 4. path enhancements. For each subpath,
    A. use membername shorthand (iff possible) to substitute for bracketed name selector. (Use MemberNameShorthand regex
    to see if the name can exist as a mnsh. If so, add that alternate path
    B. For each subpath, add variation using a wildcard selector. both as a member name shorthand and a bracketed wildcard
    selector.
    C. For lists, after visiting each element in the list to generate the subpaths for each, randomly generate a slice selector
    based on the size of the array and generate a subpath for the slice selector
    D. For each new psth segment added, also add a Descendant segment as well.
"""

_MODULE_DIR = Path(__file__).parent

FRAGILE_TEST_DIR = _MODULE_DIR / '../../tests/'
FRAGILE_TEST_DIR_PATH = Path(FRAGILE_TEST_DIR)
LEXER_TEST_CASES_FILENAME = "lexer_test_cases.json"
PARSER_TEST_CASES_FILENAME = "parser_test_cases.json"


def load_obj_from_json_file(input_file: Path) -> JSON_ValueType:
    """Return the JSON object from the JSON file in the argument.
    Intended for a JSON file with a single object for testing and debugging. """
    with open(input_file, "rb", buffering=ONE_MEBIBYTE) as in_file:
        json_str = in_file.read()
        return cast(JSON_ValueType, json.loads(json_str))
    

def write_test_case_file(outfile_path: Path, description: str, test_cases: list[Any]) -> None:
    tests_list: list[dict[str, Any]] = []
    test_file_dict = {"description":f"{description} This file is autogenerated, do not edit.",
                      "tests": tests_list }
    for test_case in test_cases:
        tests_list.append(asdict(test_case))
    
    with open(outfile_path, "x", encoding=UTF8, buffering=ONE_MEBIBYTE) as outfile:
        json.dump(test_file_dict, outfile, ensure_ascii=False, indent=4)
    
####################################################################
# LEXER TEST GENERATION
####################################################################

@dataclass(frozen=True, slots=True)
class LexerTestCase:
    test_name         : str
    json_path         : str
    lexer_tokens      : str
    source_file_name  : str
    is_invalid        : bool = False
    
    
def tokenize_jpath_str(file_name:str, jpath_query_str: str) -> str:
    """ Run the `jpath_query_str` through the lexer and return a string representation of the generated Token list. """
    lexer = JPathLexer(file_name, jpath_query_str)
    tokens, error = lexer.tokenize()
    result_str: str = ""
    if error:
        result_str = error.as_test_string()
    else:
        if tokens:
            result_str = ', '.join( t.__testrepr__() for t in tokens)
    return result_str
    

def generate_lexer_path_tokens(input_path: Path) -> list[ tuple[ str, str]]:
    """For each line in the input file of JSON Path query strings, return a list of tokens generated by the lexer.
    
    Each line of the input file is a JSON Path query string. This method will pass each string to the lexer and
    receive from it a list of scanned tokens corresponding to the input string, or an error message if the lexer failed
    to scan the input string.
    This method returns a list of two-item tuples. The first tuple item is the original JSON Path query string.
    The second tuple item is a string representation for the list of generated tokens, or a lexer error message.
    The string representation of each token is generated by calling __testrepr__() on each token.
    Lines in the input file that start with a # (line comment) are ignored, as well as blank lines.
    Note that # is a valid character that may be part of a member name or search string, so end-of-line comments
    are not supported.
    """
    result_list: list[ tuple[ str, str] ] = []
    file_name: str = input_path.name
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_str: str = tokenize_jpath_str(file_name, line_stripped)
            result_list.append( (line_stripped, result_str) )
    return result_list

def generate_lexer_test_cases(path_tokens: list[ tuple[ str, str] ],
                              source_file_name: str,
                              ) -> list[LexerTestCase]:
    
    result_list: list[LexerTestCase] = []
    for path, tokens in path_tokens:
        test_name = f"{source_file_name}-{path}"
        result_list.append( LexerTestCase(test_name, path, tokens, source_file_name) )
    return result_list


def process_lexer_paths(input_dir: Path, suffix: str, generate_test_file: bool = False, quiet:bool = False)-> None:
    """Given a directory path and a file suffix, run generate_lexer_path_tokens() for each file in the directory
    that ends with `suffix`. No directory recursion is done. Each input file contains a list of JSON Path query strings.
    
    This is a utility function to help create test data for lexer unit testing. This should be used with caution to avoid
    accidentally ovewriting existing test files. However, the open() call for creating the test file uses the 'x' mode
    flag, so it will report an error if the file already exists.
    
    If `generate_test_file` is True, create a JSON test file containing each matching input file's test data
     and save it as:  tests/jpath/lexing/lexer_test_cases.json.

    If `quiet` is True, omits writing most output to the console, except for warning or error messages.    
    """
    if not quiet:
        print(f"*** Processing jpathl files for the Lexer")
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    output_dir = FRAGILE_TEST_DIR_PATH /  "jpath/lexing" 
    if generate_test_file:
        # ensure test directory exists
        if not quiet:
            print(f"\nGenerating test files in {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
    generated_files: list[str] = []
    test_cases: list[LexerTestCase] = []
    for file in sorted(input_dir.iterdir()):
        if file.name.endswith(suffix):
            if not quiet:
                print(f"\nProcessing '{file}'")
            file_results: list[ tuple[ str, str ]] = generate_lexer_path_tokens(file)
            if not file_results:
                print(f"Warning: file '{file}' produced no results. Is it empty?")
                continue
            
            if file_results and not quiet:
                print(f"{input_dir.name}/{file.name} results:")
                for jpath, tokens in file_results:
                    print(f"{jpath}{JPATH_DATA_SEPARATOR}{tokens}")
                print("-" * 40)
                
            if generate_test_file:
                test_cases.extend(generate_lexer_test_cases(file_results, str(file.stem)))
                
    if generate_test_file and test_cases:
        outfile_path = output_dir / LEXER_TEST_CASES_FILENAME
        if not quiet: print(f"Generating test file '{outfile_path}'")
        generated_files.append( outfile_path.name )
        tests_list: list[dict[str, Any]] = []
        test_file_dict = {"description":"Lexer tests of example paths in RFC 9535 tables 2-18. This file is autogenerated, do not edit.",
                          "tests": tests_list }
        for test_case in test_cases:
            tests_list.append(asdict(test_case))
  
        with open(outfile_path, "x", encoding=UTF8, buffering=ONE_MEBIBYTE) as outfile:
            json.dump(test_file_dict, outfile, ensure_ascii=False, indent=4)

                
    if generated_files and not quiet:
        print(f"\nGenerated a total of {len(generated_files)} lexer test files:")
        for line in sorted(generated_files):
            print(f"  {line}" )


####################################################################
# PARSER TEST GENERATION
####################################################################

@dataclass(frozen=True, slots=True)
class ParserTestCase:
    test_name         : str
    json_path         : str
    parser_ast        : str
    source_file_name  : str
    is_invalid        : bool = False
    err_msg           : str  = ""
    subparse_production: str| None = None
    
    
def lex(file_name:str, jpath_query_str: str) -> tuple[ list[Token], Error | None]:
    lexer = JPathLexer(file_name, jpath_query_str)
    return lexer.tokenize()

def parse_jpath_str(file_name:str, jpath_query_str: str) -> ParserTestCase:
    """Lex and parse the JSON Path query string and return a ParserTestCase representing the query string and
     a string representation of the generated AST from the parser. Set is_invalid to True if parsing results in
     an error. """
    tokens, error = lex(file_name, jpath_query_str)
    test_name = f"{file_name}-{jpath_query_str}"
    if error:
        err_msg = error.as_test_string()
        return ParserTestCase(test_name, jpath_query_str, "", file_name, True, err_msg)
    
    parser = JPathParser(tokens)
    result: ParseResult =  parser.parse()
    if result.error:
        err_msg = result.error.as_test_string()
        return ParserTestCase(test_name, jpath_query_str, "", file_name, True, err_msg)
    
    ast: ASTNode| None  = result.node
    ast_str = str(ast)
    return ParserTestCase(test_name, jpath_query_str, ast_str, file_name, False, '')

def subparse_jpath_str(file_name:str, jpath_query_str: str, production_name: str) -> ParserTestCase:
    tokens, error = lex(file_name, jpath_query_str)
    test_name = f"{file_name}-{jpath_query_str}"
    if error:
        err_msg = error.as_test_string()
        return ParserTestCase(test_name, jpath_query_str, "", file_name, True, err_msg, production_name)
    
    parser = JPathParser(tokens)
    result: tuple[ list[ tuple[str, ASTNode] ] , list[ tuple[str, Error] ] ] =  parser.subparse(production_name)
    node_list = result[0]
    ast_str = ""
    err_list = result[1]
    err_msg = ""
    for name, error in err_list:
        if name == production_name:
            err_msg = error.as_test_string()
            
    if err_msg:
        return ParserTestCase(test_name, jpath_query_str, "", file_name, True, err_msg, production_name)
    
    for name, ast_node in node_list:
        if name == production_name:
            ast_str = str(ast_node)

    return ParserTestCase(test_name, jpath_query_str, ast_str, file_name, False, '', production_name)

def generate_parser_test_cases(input_path: Path) -> list[ ParserTestCase]:
    """For each line in the input file of JSON Path query strings, return a ParserTestCase
    
    Each line of the input file is a JSON Path query string. This method will pass each string to parse_jpath_str and
    receive from it a ParserTestCase containing a representation of the generated ASTs corresponding to the input string,
    or an error message if the parser failed to parse the input string.

    Lines in the input file that start with a # (line comment) are ignored, as well as blank lines.
    Note that # is a valid character that may be part of a member name or search string, so end-of-line comments
    are not supported.
    """
    result_list: list[ ParserTestCase ] = []
    file_name: str = input_path.stem
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_list.append( parse_jpath_str(file_name, line_stripped) )
    return result_list

def generate_subparser_test_cases(input_path: Path,
                                  output_path:Path,
                                  production_name: str,
                                  generate_test_file = False,
                                  quiet: bool = False) -> None:
    """Generate a test case for the input file by calling subparse(production_name) """
    test_cases: list[ ParserTestCase] = []
    file_name: str = input_path.stem
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            test_cases.append(subparse_jpath_str(file_name, line_stripped, production_name))
            
    if not quiet:
        display_test_cases(test_cases, str(input_path))
    
    if generate_test_file and test_cases:
        if not quiet: print(f"Generating test file '{output_path}'")
        write_test_case_file(output_path, "Parser subparse tests of example paths in RFC 9535 table 11.", test_cases)
    
        

def display_test_cases(test_cases: list[ Any ], file_name: str) -> None:
    """Display the generated test cases during test generation"""
    if test_cases:
        print(f"{file_name} results:")
        for test_case in test_cases:
            if is_dataclass(test_case):
                dict_ = asdict(test_case)
                value = dict_.get("lexer_tokens", None) or dict_.get("parser_ast", None) or ''
                msg = dict_["err_msg"] if dict_["is_invalid"] else value
                print(f"{dict_['json_path']}{JPATH_DATA_SEPARATOR}{msg}")
            else:
                jpath, expected = test_case
                print(f"{jpath}{JPATH_DATA_SEPARATOR}{expected}")
        print("-" * 40)


def process_parser_paths(input_dir: Path,
                         suffix: str,
                         output_dir: Path | None = None,
                         generate_test_file: bool = False,
                         quiet:bool = False)-> None:
    
    """Given a directory path and a file suffix, run generate_parser_path_asts() for each file in the directory
    that ends with `suffix`. No directory recursion is done. Each input file contains a list of JSON Path query strings.
    
    This is a utility function to help create test data for parser unit testing. This should be used with caution to avoid
    accidentally ovewriting existing test files. However, the open() call for creating the test file uses the 'x' mode
    flag, so it will report an error if the file already exists.
    
    If `generate_test_file` is True, create a JSON test file containing each matching input file's test data
     and save it as:  tests/jpath/parsing/parser_test_cases.json.
    
    If `quiet` is True, omits writing most output to the console, except for warning or error messages.
    """
    if not quiet:
        print(f"*** Processing jpathl files for the Parser")
        
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    if generate_test_file and output_dir is None:
        raise ValueError(f"`generate_test_files` is  True yet no output directory was specified.")
    
    if generate_test_file and output_dir is not None:
        # ensure test directory exists
        output_dir.mkdir(parents=True, exist_ok=True)
        if not quiet:
            print(f"\nGenerating test files in {output_dir}")
    
    generated_files: list[str] = []
    test_cases: list[ParserTestCase] = []
    for file in sorted(input_dir.iterdir()):
        if file.name.endswith(suffix):
            if not quiet:
                print(f"\nProcessing '{file}'")
            file_results: list[ ParserTestCase  ] = generate_parser_test_cases(file)
            if not file_results:
                print(f"Warning: file '{file}' produced no results. Is it empty?")
                continue
                
            if not quiet:
                display_test_cases(file_results, f"{input_dir.name}/{file.name}")
            
            if generate_test_file:
                test_cases.extend(file_results)
    
    if generate_test_file and test_cases:
        outfile_path = output_dir / PARSER_TEST_CASES_FILENAME  # type: ignore
        if not quiet: print(f"Generating test file '{outfile_path}'")
        generated_files.append( outfile_path.name )
        write_test_case_file(outfile_path, "Parser tests of example paths in RFC 9535 tables 2-18.", test_cases)
    
    if generated_files and not quiet:
        print(f"\nGenerated a total of {len(generated_files)} parser test files:")
        for line in sorted(generated_files):
            print(f"  {line}" )


####################################################################
# EVALUATOR TEST GENERATION
####################################################################

def evaluate_jpath_str(file_name:str, jpath_query_str: str, json_value:JSON_ValueType) -> VNodeList:
    """Lex and parse the JSON Path query string and evalutate it with the JSON value in the argument.
    Return a VNodeList"""
    lexer = JPathLexer(file_name, jpath_query_str)
    tokens, error = lexer.tokenize()
    if error:
        result_str = error.as_test_string()
        raise ValueError(result_str)
    
    parser = JPathParser(tokens)
    result: ParseResult =  parser.parse()
    if result.error:
        result_str = result.error.as_test_string()
        raise ValueError(result_str)
    
    ast_node = result.node
    if ast_node is None:
        raise ValueError(f"Parser returned an empty AST for query string: {jpath_query_str}")
    context = Context('<root>')
    context.set_symbol(ROOT_JSON_VALUE_KEY, json_value)
    evaluator = JPathEvaluator()
    rt_result:RuntimeResult = evaluator.visit(ast_node, context)
    if rt_result.error:
        raise ValueError(f"Evaluator returned Error: {rt_result.error} for query string: {jpath_query_str}")
    if rt_result.value is not None:
       return rt_result.value
    else:
        raise RuntimeError(f"Evaluator returned null value for query string: {jpath_query_str}")

def generate_evaluator_path_nodelist(input_path: Path, json_value: JSON_ValueType) -> list[ tuple[ str, VNodeList]  ]:
    result_list: list[ tuple[ str, VNodeList] ] = []
    file_name: str = input_path.name
    with open(input_path, "r", encoding=UTF8, buffering=ONE_MEBIBYTE) as input_file:
        for line in input_file:
            line_stripped = line.strip()
            if line_stripped == '' or line_stripped.startswith("#"):
                continue  # ignore comment lines or blank lines
            result_nodelist: VNodeList = evaluate_jpath_str(file_name, line_stripped, json_value)
            result_list.append( (line_stripped, result_nodelist) )
    return result_list

##################################################################################################################


def rename_file_suffix(input_dir: Path, from_suffix: str, to_suffix: str)-> None:
    """ TEMP CODE """
    if not input_dir.is_dir():
        raise FileNotFoundError(f'Input directory {input_dir} does not exist')
    for file in input_dir.iterdir():
        if file.name.endswith(from_suffix):
            file.rename(file.with_suffix(to_suffix))

def create_lexer_test_files() -> None:
    """PRODUCTION CODE"""
    input_dir = Path(FRAGILE_TEST_DIR) / "jpath/rfc9535_examples/"
    process_lexer_paths(input_dir, "jpathl", generate_test_file=True)
    
def create_parser_test_files() -> None:
    """PRODUCTION CODE"""
    input_dir  = FRAGILE_TEST_DIR_PATH / "jpath/rfc9535_examples/"
    output_dir = FRAGILE_TEST_DIR_PATH / "jpath/parsing/"
    process_parser_paths(input_dir=input_dir, suffix="jpathl", output_dir=output_dir, generate_test_file=True)
    
def create_subparser_test_files() -> None:
    """PRODUCTION CODE"""
    input_path  = FRAGILE_TEST_DIR_PATH / "jpath/rfc9535_examples/table_11.jpathl"
    output_path = FRAGILE_TEST_DIR_PATH / "jpath/parsing/subparse_test_cases_table_11.json"
    generate_subparser_test_cases(
        input_path=input_path,
        output_path=output_path,
        production_name="comparison_expr",
        generate_test_file=True
    )

def create_all_test_files() -> None:
    """PRODUCTION CODE"""
    create_lexer_test_files()
    create_parser_test_files()
    create_subparser_test_files()

def main() -> None:
    #create_lexer_test_files()
    #create_parser_test_files()
    create_subparser_test_files()
    
if __name__ == '__main__':
    main()
